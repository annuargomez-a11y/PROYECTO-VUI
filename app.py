import os
import sys
import logging
import streamlit as st
import nest_asyncio

# --- PARCHES CR√çTICOS (¬°No tocar!) ---
nest_asyncio.apply()
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    StorageContext,
    load_index_from_storage,
    Settings
)
from llama_index.core.node_parser import SentenceSplitter 

from llama_index.llms.google_genai import GoogleGenAI
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# --- CONFIGURACI√ìN DE P√ÅGINA ---
st.set_page_config(
    page_title="Asistente Janus (VUI)",
    page_icon="üóùÔ∏è",
    layout="centered" 
)

# --- CONFIGURACI√ìN DE API ---
if "GOOGLE_API_KEY" in st.secrets:
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
else:
    st.error("Error: Falta la clave API de Google. Config√∫rala en los 'Secrets' de Streamlit.")
    st.stop() 

pdf_folder_path = "./ARCHIVOS/"
persist_dir = "./storage" # (Streamlit Cloud reconstruye esto, as√≠ que no es persistente)

# --- FUNCI√ìN DEL MOTOR RAG (¬°ACTUALIZADA!) ---
@st.cache_resource
def get_query_engine():
    """
    Carga o crea el √≠ndice vectorial y devuelve un motor de consulta.
    """
    
    # Configura el "Cerebro" (LLM - Google)
    llm = GoogleGenAI(model="models/gemini-pro-latest")
    
    # --- ¬°VOLVEMOS AL "TRADUCTOR" LIGERO! ---
    # Este modelo S√ç cabe en la memoria gratuita de Streamlit.
    embed_model = HuggingFaceEmbedding(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2", 
        device="cpu" 
    )

    Settings.llm = llm
    Settings.embed_model = embed_model
    
    print("Creando √≠ndice desde cero (ejecuci√≥n en la nube)...")
    
    reader = SimpleDirectoryReader(input_dir=pdf_folder_path, recursive=True)
    documents = reader.load_data()
    print(f"Se cargaron {len(documents)} documentos.")
    
    # Usamos el "Corte Inteligente"
    print("Analizando y cortando los documentos en p√°rrafos inteligentes...")
    node_parser = SentenceSplitter(
        chunk_size=1024,
        chunk_overlap=100
    )
    nodes = node_parser.get_nodes_from_documents(documents, show_progress=True)
    print(f"Se crearon {len(nodes)} trozos (nodos) de texto inteligente.")
    
    print("Creando √≠ndice (esto puede tardar unos minutos)...")
    index = VectorStoreIndex(
        nodes, 
        show_progress=True, 
        embed_batch_size=100 # Lo mantenemos en lotes
    )
    
    print("¬°√çndice creado exitosamente en memoria!")
    query_engine = index.as_query_engine(similarity_top_k=3) 
    print("¬°Sistema listo para responder!")
    return query_engine

# --- INTERFAZ DE USUARIO "ASISTENTE JANUS" ---

# --- 1. Cabecera (Sin cambios) ---
st.title("Asistente Janus")
st.caption("Tu gu√≠a para la Ventanilla √önica de Inversi√≥n (VUI).")

# --- 2. Pesta√±as de Funciones (Sin cambios) ---
tab_chat, tab_acerca_de = st.tabs(["Conversar con Janus üí¨", "Acerca de este Prototipo ‚ÑπÔ∏è"])

# --- Pesta√±a 1: El Chat (¬°ACTUALIZADA!) ---
with tab_chat:
    
    # Inicializa el saludo de Janus
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "¬°Hola! Soy Janus, tu asistente virtual. ¬°Estoy aqu√≠ para guiarte en tu Inversi√≥n Directa en Colombia!"}
        ]

    # --- ¬°INTERFAZ CORREGIDA! ---
    # Creamos un contenedor con altura fija para el historial
    chat_container = st.container(height=500) # Puedes ajustar el 500

    # Muestra los mensajes antiguos DENTRO del contenedor
    with chat_container:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

    # Carga el motor de consulta
    try:
        query_engine = get_query_engine()
    except Exception as e:
        st.error(f"Error al cargar el motor del asistente: {e}")
        st.stop()

    # Caja de chat (Queda FUERA del contenedor, fija al fondo de la pesta√±a)
    if prompt := st.chat_input("Preg√∫ntale a Janus sobre la Gu√≠a Legal..."):
        
        # A√±ade el prompt al historial de estado
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Muestra el prompt del usuario DENTRO del contenedor
        with chat_container:
            with st.chat_message("user"):
                st.markdown(prompt)

        # Genera y muestra la respuesta DENTRO del contenedor
        with chat_container:
            with st.chat_message("assistant"):
                with st.spinner("Consultando la Gu√≠a Legal y contactando a Gemini..."):
                    try:
                        respuesta = query_engine.query(prompt)
                        response_text = str(respuesta)
                    except Exception as e:
                        response_text = f"Error al contactar a Gemini: {e}. Por favor, espera unos segundos e int√©ntalo de nuevo."
                
                st.markdown(response_text)
        
        # A√±ade la respuesta al historial de estado
        st.session_state.messages.append({"role": "assistant", "content": response_text})

# --- Pesta√±a 2: Informaci√≥n (Sin cambios) ---
with tab_acerca_de:
    st.header("Sobre este Prototipo")
    # ... (El resto del c√≥digo de la pesta√±a 2) ...
    st.markdown("""
    Este es un prototipo RAG (Generaci√≥n Aumentada por RecuperACI√ìN)
    con "Corte Inteligente" (Smart Chunking).
    
    **Tecnolog√≠as utilizadas:**
    * **Interfaz:** Streamlit
    * **Orquestador RAG:** LlamaIndex
    * **Cerebro (LLM):** Google Gemini (`gemini-pro-latest`)
    * **Traductor (Embedding):** `paraphrase-multilingual-MiniLM-L12-v2` (Local/CPU)
* **Base de Conocimiento:** 14 PDFs de la Gu√≠a Legal 2025.
    """)
    st.warning("El arranque inicial de esta aplicaci√≥n tarda 2-3 minutos mientras se crea el √≠ndice de los PDFs.")
